#!/bin/bash

# set -e

# Init ZooKeeper data directory if it does not exist
if [ ! "$(ls -A /var/lib/zookeeper)" ]; then
  zookeeper-server-initialize
fi

# Configure HDFS if it is the first start
hostname=$(hostname -f)
ip=$(hostname -i)
if [ ! -d "/etc/hadoop/conf.$hostname" ]; then
  # Create Hadoop configuration directory
  cp -r /etc/hadoop/conf.empty /etc/hadoop/conf.$hostname
  update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.$hostname 50
  update-alternatives --set hadoop-conf /etc/hadoop/conf.$hostname

  # Create configuration files
  printf "\
  <configuration>\n\
    <property>\n\
      <name>fs.defaultFS</name>\n\
      <value>hdfs://$ip:8020</value>\n\
    </property>\n\
    <property>\n\
      <name>hadoop.proxyuser.hue.hosts</name>\n\
      <value>*</value>\n\
    </property>\n\
    <property>\n\
      <name>hadoop.proxyuser.hue.groups</name>\n\
      <value>*</value>\n\
    </property>\n\
    <property>\n\
      <name>hadoop.proxyuser.httpfs.hosts</name>\n\
      <value>*</value>\n\
    </property>\n\
    <property>\n\
      <name>hadoop.proxyuser.httpfs.groups</name>\n\
      <value>*</value>\n\
    </property>\n\
  </configuration>\n\
  " > /etc/hadoop/conf.$hostname/core-site.xml
  cp ~/hdfs-site.xml /etc/hadoop/conf.$hostname/
  cp ~/mapred-site.xml /etc/hadoop/conf.$hostname/
  printf "\
  <configuration>\n\
    <property>\n\
      <name>yarn.resourcemanager.hostname</name>\n\
      <value>$(hostname -f)</value>\n\
    </property>\n\
    <property>\n\
      <description>Classpath for typical applications.</description>\n\
      <name>yarn.application.classpath</name>\n\
      <value>\n\
          $HADOOP_CONF_DIR,\n\
          $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,\n\
          $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,\n\
          $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,\n\
          $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*\n\
      </value>\n\
    </property>\n\
    <property>\n\
      <name>yarn.nodemanager.aux-services</name>\n\
      <value>mapreduce_shuffle</value>\n\
    </property>\n\
    <property>\n\
      <name>yarn.nodemanager.local-dirs</name>\n\
      <value>file:///data/1/yarn/local,file:///data/2/yarn/local,file:///data/3/yarn/local</value>\n\
    </property>\n\
    <property>\n\
      <name>yarn.nodemanager.log-dirs</name>\n\
      <value>file:///data/1/yarn/logs,file:///data/2/yarn/logs,file:///data/3/yarn/logs</value>\n\
    </property>\n\
    <property>\n\
      <name>yarn.log.aggregation-enable</name>\n\
      <value>true</value>\n\
    </property>\n\
    <property>\n\
      <description>Where to aggregate logs</description>\n\
      <name>yarn.nodemanager.remote-app-log-dir</name>\n\
      <value>hdfs://$ip:8020/var/log/hadoop-yarn/apps</value>\n\
    </property>\n\
  </configuration>\n\
  " > /etc/hadoop/conf.$hostname/yarn-site.xml

  # Create data directories
  mkdir -p /data/1/dfs/nn /nfsmount/dfs/nn
  chown -R hdfs:hdfs /data/1/dfs/nn /nfsmount/dfs/nn
  chmod 700 /data/1/dfs/nn /nfsmount/dfs/nn
  chmod go-rx /data/1/dfs/nn /nfsmount/dfs/nn

  # Create YARN directories
  mkdir -p /data/1/yarn/local /data/2/yarn/local /data/3/yarn/local /data/4/yarn/local
  mkdir -p /data/1/yarn/logs /data/2/yarn/logs /data/3/yarn/logs /data/4/yarn/logs
  chown -R yarn:yarn /data/1/yarn/local /data/2/yarn/local /data/3/yarn/local /data/4/yarn/local
  chown -R yarn:yarn /data/1/yarn/logs /data/2/yarn/logs /data/3/yarn/logs /data/4/yarn/logs

  # Fomat NameNode
  su hdfs -c "echo Y | hdfs namenode -format"
fi

# Start ZooKeeper
zookeeper-server start

# Start YARN
/etc/init.d/hadoop-yarn-resourcemanager start

# Start HDFS
/etc/init.d/hadoop-hdfs-namenode start

# DinD: a wrapper script which allows docker to be run inside a docker container.
# Original version by Jerome Petazzoni <jerome@docker.com>
# See the blog post: https://blog.docker.com/2013/09/docker-can-now-run-within-docker/
#
# This script should be executed inside a docker container in privilieged mode
# ('docker run --privileged', introduced in docker 0.6).

# Usage: dind CMD [ARG...]

# apparmor sucks and Docker needs to know that it's in a container (c) @tianon
export container=docker

# as of docker 1.8, cgroups will be mounted in the container
if ! mountpoint -q /sys/fs/cgroup; then

	# First, make sure that cgroups are mounted correctly.
	CGROUP=/cgroup

	mkdir -p "$CGROUP"

	if ! mountpoint -q "$CGROUP"; then
		mount -n -t tmpfs -o uid=0,gid=0,mode=0755 cgroup $CGROUP || {
			echo >&2 'Could not make a tmpfs mount. Did you use --privileged?'
			exit 1
		}
	fi

	# Mount the cgroup hierarchies exactly as they are in the parent system.
	for HIER in $(cut -d: -f2 /proc/1/cgroup); do

		SUBSYSTEMS="${HIER%name=*}"

		# If cgroup hierarchy is named(mounted with "-o name=foo") we
		# need to mount it in $CGROUP/foo to create exect same
		# directoryes as on host. Else we need to mount it as is e.g.
		# "subsys1,subsys2" if it has two subsystems

		# Named, control-less cgroups are mounted with "-o name=foo"
		# (and appear as such under /proc/<pid>/cgroup) but are usually
		# mounted on a directory named "foo" (without the "name=" prefix).
		# Systemd and OpenRC (and possibly others) both create such a
		# cgroup. So just mount them on directory $CGROUP/foo.

		OHIER=$HIER
		HIER="${HIER#*name=}"

		mkdir -p "$CGROUP/$HIER"

		if ! mountpoint -q "$CGROUP/$HIER"; then
			mount -n -t cgroup -o "$OHIER" cgroup "$CGROUP/$HIER"
		fi

		# Likewise, on at least one system, it has been reported that
		# systemd would mount the CPU and CPU accounting controllers
		# (respectively "cpu" and "cpuacct") with "-o cpuacct,cpu"
		# but on a directory called "cpu,cpuacct" (note the inversion
		# in the order of the groups). This tries to work around it.

		if [ "$HIER" = 'cpuacct,cpu' ]; then
			ln -s "$HIER" "$CGROUP/cpu,cpuacct"
		fi

		# If hierarchy has multiple subsystems, in /proc/<pid>/cgroup
		# we will see ":subsys1,subsys2,subsys3,name=foo:" substring,
		# we need to mount it to "$CGROUP/foo" and if there were no
		# name to "$CGROUP/subsys1,subsys2,subsys3", so we must create
		# symlinks for docker daemon to find these subsystems:
		# ln -s $CGROUP/foo $CGROUP/subsys1
		# ln -s $CGROUP/subsys1,subsys2,subsys3 $CGROUP/subsys1

		if [ "$SUBSYSTEMS" != "${SUBSYSTEMS//,/ }" ]; then
			SUBSYSTEMS="${SUBSYSTEMS//,/ }"
			for SUBSYS in $SUBSYSTEMS
			do
				ln -s "$CGROUP/$HIER" "$CGROUP/$SUBSYS"
			done
		fi
	done
fi

if [ -d /sys/kernel/security ] && ! mountpoint -q /sys/kernel/security; then
	mount -t securityfs none /sys/kernel/security || {
		echo >&2 'Could not mount /sys/kernel/security.'
		echo >&2 'AppArmor detection and --privileged mode might break.'
	}
fi

# Note: as I write those lines, the LXC userland tools cannot setup
# a "sub-container" properly if the "devices" cgroup is not in its
# own hierarchy. Let's detect this and issue a warning.
if ! grep -q :devices: /proc/1/cgroup; then
	echo >&2 'WARNING: the "devices" cgroup should be in its own hierarchy.'
fi
if ! grep -qw devices /proc/1/cgroup; then
	echo >&2 'WARNING: it looks like the "devices" cgroup is not mounted.'
fi

# Mount /tmp (conditionally)
if ! mountpoint -q /tmp; then
	mount -t tmpfs none /tmp
fi

rm /var/run/docker.pid
ps axf | grep docker | grep -v grep | awk '{print "kill -9 " $1}' | sh
/etc/init.d/docker start
mesos-slave "${@:2}"
